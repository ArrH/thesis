{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassicML_SC_DEAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_-ry0NAZ0da_PQfHOZDIkz0qtiY3lv7a",
      "authorship_tag": "ABX9TyPgvSvjaXrPX9pepWkMlAIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArrH/thesis/blob/main/ClassicML_SC_DEAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTebM1AuE5Y"
      },
      "source": [
        "In this file we evaluate the performance of the following strategy: \n",
        "\n",
        "1. We prepare and normalize the raw GSR signal from the DEAP dataset to be representative of the wearable device acquired Skin Conductivity signal.\n",
        "2. We get the phasic and tonic components of each signal.\n",
        "3. We process the labels.\n",
        "4. We extract features from the signal.\n",
        "5. We select features related to labels.\n",
        "6. We test a bunch of algos on these and store the results. We use 10 fold cross validation to assess the model performance with better statistical significance. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UeajB5BXbyd"
      },
      "source": [
        "Let's start by extracting relevant data from the raw data files. \n",
        "\n",
        "Whe check if the files exists, if not, we extract and save for the use in the future runs of this code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8D-beUluC86"
      },
      "source": [
        "try:\n",
        "   import cPickle as pickle\n",
        "except:\n",
        "   import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "subject_count = 32\n",
        "trial_count_per_subject = 40\n",
        "sc_index = 36\n",
        "raw_data_sample_rate = 128\n",
        "signal_length = 8064\n",
        "\n",
        "\n",
        "def get_raw_data(DEAP_data_directory):\n",
        "  if os.path.isfile(DEAP_data_directory + '/sc_array_raw'):\n",
        "    print('Raw data exists numpy stored format already. Using that.')\n",
        "    with open(DEAP_data_directory + '/sc_array_raw', 'rb') as f:\n",
        "      sc_array_raw = np.array(pickle.load(f))\n",
        "    with open(DEAP_data_directory + '/valence_array_raw', 'rb') as f:\n",
        "      valence_array_raw = np.array(pickle.load(f))\n",
        "    with open(DEAP_data_directory + '/arousal_array_raw', 'rb') as f:\n",
        "      arousal_array_raw = np.array(pickle.load(f))\n",
        "    with open(DEAP_data_directory + '/dominance_array_raw', 'rb') as f:\n",
        "      dominance_array_raw = np.array(pickle.load(f))\n",
        "    return sc_array_raw, valence_array_raw, arousal_array_raw, dominance_array_raw\n",
        "  else:\n",
        "    print('Opening raw data. Might take a while')\n",
        "    sc_array_raw = np.empty(shape=(subject_count*trial_count_per_subject, signal_length))\n",
        "    valence_array_raw = np.empty(shape=(subject_count*trial_count_per_subject))\n",
        "    arousal_array_raw = np.empty(shape=(subject_count*trial_count_per_subject))\n",
        "    dominance_array_raw = np.empty(shape=(subject_count*trial_count_per_subject))\n",
        "    counter = 0\n",
        "    for subject in range(1, subject_count+1):\n",
        "      print('Opening File of Subject ', subject, '...')\n",
        "      if subject < 10:\n",
        "          subject_raw_data_dir = DEAP_data_directory + \"/raw_data/s0{subject_number}.dat\".format(subject_number=subject)\n",
        "      else:\n",
        "          subject_raw_data_dir = DEAP_data_directory + \"/raw_data/s{subject_number}.dat\".format(subject_number=subject)\n",
        "      with open(subject_raw_data_dir, 'rb') as f:\n",
        "          d = pickle.load(f, encoding='latin1')\n",
        "          for trial in range(trial_count_per_subject):\n",
        "              sc_array_raw[counter] =d['data'][trial][sc_index]\n",
        "              valence_array_raw[counter] = d['labels'][trial][0]\n",
        "              arousal_array_raw[counter] = d['labels'][trial][1]\n",
        "              dominance_array_raw[counter] = d['labels'][trial][2]\n",
        "              counter += 1\n",
        "                     \n",
        "    with open(DEAP_data_directory + '/sc_array_raw', 'wb') as f:\n",
        "      pickle.dump(sc_array_raw, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
        "    with open(DEAP_data_directory + '/valence_array_raw', 'wb') as f:\n",
        "      pickle.dump(valence_array_raw, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
        "    with open(DEAP_data_directory + '/arousal_array_raw', 'wb') as f:\n",
        "      pickle.dump(arousal_array_raw, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
        "    with open(DEAP_data_directory + '/dominance_array_raw', 'wb') as f:\n",
        "      pickle.dump(dominance_array_raw, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
        "\n",
        "    return sc_array_raw, valence_array_raw, arousal_array_raw, dominance_array_raw  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sneNwrXKdSpH"
      },
      "source": [
        "Now, let's make a function that processes the SC data to be representative of a wearable device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APjEKjXudi07"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, resample\n",
        "\n",
        "def filter_sc_signal_noise(sc_signal, cutoff_frequency, fs=None):\n",
        "    if fs is None:\n",
        "        fs = 128\n",
        "    cutoff = cutoff_frequency\n",
        "    order = 2\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    sc_signal_filtered = filtfilt(b, a, sc_signal)\n",
        "    return sc_signal_filtered\n",
        "\n",
        "\n",
        "def subsample(signal, desired_frequency, cutoff_begin, cutoff_end):\n",
        "    signal = resample(signal, num=63*desired_frequency)\n",
        "    return signal[cutoff_begin*desired_frequency:-cutoff_end*desired_frequency]\n",
        "\n",
        "def process_data(sc_array_raw, cutoff_frequency, desired_sample_frequency):\n",
        "  sc_array_processed = np.empty(shape=(len(sc_array_raw), 60*desired_sample_frequency))\n",
        "  # First, let' filter and subsample the signal\n",
        "  for signal_no in range(len(sc_array_raw)):\n",
        "    signal = sc_array_raw[signal_no]\n",
        "    signal = filter_sc_signal_noise(sc_signal=signal, cutoff_frequency=cutoff_frequency)\n",
        "    signal = subsample(signal=signal, desired_frequency=desired_sample_frequency, cutoff_begin=2, cutoff_end=1)\n",
        "    sc_array_processed[signal_no] = signal\n",
        "  # Now, lets get the maximum SC value per subject and use that to normalize the signal\n",
        "  max_sc_list = []\n",
        "  for signal_number in range(0, len(sc_array_raw), trial_count_per_subject):\n",
        "    signals = sc_array_processed[signal_number:(signal_number+trial_count_per_subject)]\n",
        "    sc_max = np.amax(signals)\n",
        "    max_sc_list.append(sc_max)\n",
        "    sc_array_processed[signal_number:(signal_number+trial_count_per_subject)] = sc_array_processed[signal_number:(signal_number+40)]/sc_max\n",
        "\n",
        "  return sc_array_processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8tZkj1PsLsu"
      },
      "source": [
        "Now, lets get the Tonic and Phasic Components From The Signals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2IcB4zdsRSI"
      },
      "source": [
        "\"\"\"\n",
        "______________________________________________________________________________\n",
        " File:                         cvxEDA.py\n",
        " Last revised:                 07 Nov 2015 r69\n",
        " ______________________________________________________________________________\n",
        " Copyright (C) 2014-2015 Luca Citi, Alberto Greco\n",
        " \n",
        " This program is free software; you can redistribute it and/or modify it under\n",
        " the terms of the GNU General Public License as published by the Free Software\n",
        " Foundation; either version 3 of the License, or (at your option) any later\n",
        " version.\n",
        " \n",
        " This program is distributed in the hope that it will be useful, but WITHOUT\n",
        " ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
        " FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
        " \n",
        " You may contact the author by e-mail (lciti@ieee.org).\n",
        " ______________________________________________________________________________\n",
        " This method was first proposed in:\n",
        " A Greco, G Valenza, A Lanata, EP Scilingo, and L Citi\n",
        " \"cvxEDA: a Convex Optimization Approach to Electrodermal Activity Processing\"\n",
        " IEEE Transactions on Biomedical Engineering, 2015\n",
        " DOI: 10.1109/TBME.2015.2474131\n",
        " If you use this program in support of published research, please include a\n",
        " citation of the reference above. If you use this code in a software package,\n",
        " please explicitly inform the end users of this copyright notice and ask them\n",
        " to cite the reference above in their published research.\n",
        " ______________________________________________________________________________\n",
        "\"\"\"\n",
        "\n",
        "import cvxopt as cv\n",
        "import cvxopt.solvers\n",
        "\n",
        "def cvxEDA(y, delta, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2,\n",
        "           solver=None, options={'reltol':1e-9}):\n",
        "    \"\"\"CVXEDA Convex optimization approach to electrodermal activity processing\n",
        "    This function implements the cvxEDA algorithm described in \"cvxEDA: a\n",
        "    Convex Optimization Approach to Electrodermal Activity Processing\"\n",
        "    (http://dx.doi.org/10.1109/TBME.2015.2474131, also available from the\n",
        "    authors' homepages).\n",
        "    Arguments:\n",
        "       y: observed EDA signal (we recommend normalizing it: y = zscore(y))\n",
        "       delta: sampling interval (in seconds) of y\n",
        "       tau0: slow time constant of the Bateman function\n",
        "       tau1: fast time constant of the Bateman function\n",
        "       delta_knot: time between knots of the tonic spline function\n",
        "       alpha: penalization for the sparse SMNA driver\n",
        "       gamma: penalization for the tonic spline coefficients\n",
        "       solver: sparse QP solver to be used, see cvxopt.solvers.qp\n",
        "       options: solver options, see:\n",
        "                http://cvxopt.org/userguide/coneprog.html#algorithm-parameters\n",
        "    Returns (see paper for details):\n",
        "       r: phasic component\n",
        "       p: sparse SMNA driver of phasic component\n",
        "       t: tonic component\n",
        "       l: coefficients of tonic spline\n",
        "       d: offset and slope of the linear drift term\n",
        "       e: model residuals\n",
        "       obj: value of objective function being minimized (eq 15 of paper)\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(y)\n",
        "    y = cv.matrix(y)\n",
        "\n",
        "    # bateman ARMA model\n",
        "    a1 = 1./min(tau1, tau0) # a1 > a0\n",
        "    a0 = 1./max(tau1, tau0)\n",
        "    ar = np.array([(a1*delta + 2.) * (a0*delta + 2.), 2.*a1*a0*delta**2 - 8.,\n",
        "        (a1*delta - 2.) * (a0*delta - 2.)]) / ((a1 - a0) * delta**2)\n",
        "    ma = np.array([1., 2., 1.])\n",
        "\n",
        "    # matrices for ARMA model\n",
        "    i = np.arange(2, n)\n",
        "    A = cv.spmatrix(np.tile(ar, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
        "    M = cv.spmatrix(np.tile(ma, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
        "\n",
        "    # spline\n",
        "    delta_knot_s = int(round(delta_knot / delta))\n",
        "    spl = np.r_[np.arange(1.,delta_knot_s), np.arange(delta_knot_s, 0., -1.)] # order 1\n",
        "    spl = np.convolve(spl, spl, 'full')\n",
        "    spl /= max(spl)\n",
        "    # matrix of spline regressors\n",
        "    i = np.c_[np.arange(-(len(spl)//2), (len(spl)+1)//2)] + np.r_[np.arange(0, n, delta_knot_s)]\n",
        "    nB = i.shape[1]\n",
        "    j = np.tile(np.arange(nB), (len(spl),1))\n",
        "    p = np.tile(spl, (nB,1)).T\n",
        "    valid = (i >= 0) & (i < n)\n",
        "    B = cv.spmatrix(p[valid], i[valid], j[valid])\n",
        "\n",
        "    # trend\n",
        "    C = cv.matrix(np.c_[np.ones(n), np.arange(1., n+1.)/n])\n",
        "    nC = C.size[1]\n",
        "\n",
        "    # Solve the problem:\n",
        "    # .5*(M*q + B*l + C*d - y)^2 + alpha*sum(A,1)*p + .5*gamma*l'*l\n",
        "    # s.t. A*q >= 0\n",
        "\n",
        "    old_options = cv.solvers.options.copy()\n",
        "    cv.solvers.options.clear()\n",
        "    cv.solvers.options.update(options)\n",
        "    if solver == 'conelp':\n",
        "        # Use conelp\n",
        "        z = lambda m,n: cv.spmatrix([],[],[],(m,n))\n",
        "        G = cv.sparse([[-A,z(2,n),M,z(nB+2,n)],[z(n+2,nC),C,z(nB+2,nC)],\n",
        "                    [z(n,1),-1,1,z(n+nB+2,1)],[z(2*n+2,1),-1,1,z(nB,1)],\n",
        "                    [z(n+2,nB),B,z(2,nB),cv.spmatrix(1.0, range(nB), range(nB))]])\n",
        "        h = cv.matrix([z(n,1),.5,.5,y,.5,.5,z(nB,1)])\n",
        "        c = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T,z(nC,1),1,gamma,z(nB,1)])\n",
        "        res = cv.solvers.conelp(c, G, h, dims={'l':n,'q':[n+2,nB+2],'s':[]})\n",
        "        obj = res['primal objective']\n",
        "    else:\n",
        "        # Use qp\n",
        "        Mt, Ct, Bt = M.T, C.T, B.T\n",
        "        H = cv.sparse([[Mt*M, Ct*M, Bt*M], [Mt*C, Ct*C, Bt*C], \n",
        "                    [Mt*B, Ct*B, Bt*B+gamma*cv.spmatrix(1.0, range(nB), range(nB))]])\n",
        "        f = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T - Mt*y,  -(Ct*y), -(Bt*y)])\n",
        "        res = cv.solvers.qp(H, f, cv.spmatrix(-A.V, A.I, A.J, (n,len(f))),\n",
        "                            cv.matrix(0., (n,1)), solver=solver)\n",
        "        obj = res['primal objective'] + .5 * (y.T * y)\n",
        "    cv.solvers.options.clear()\n",
        "    cv.solvers.options.update(old_options)\n",
        "\n",
        "    l = res['x'][-nB:]\n",
        "    d = res['x'][n:n+nC]\n",
        "    t = B*l + C*d\n",
        "    q = res['x'][:n]\n",
        "    p = A * q\n",
        "    r = M * q\n",
        "    e = y - r - t\n",
        "\n",
        "    return (np.array(a).ravel() for a in (r, p, t, l, d, e, obj))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_ae8rhLtrLx"
      },
      "source": [
        "def apply_cvx(sc_array_processed):\n",
        "  phasic_sc_array = np.empty(shape=(1280, 60*desired_sample_frequency))\n",
        "  tonic_sc_array = np.empty(shape=(1280, 60*desired_sample_frequency))\n",
        "\n",
        "  for i in range(len(sc_array_processed)):\n",
        "    [r, p, t, l, d, e, obj] = cvxEDA(sc_array_processed[i], 1./desired_sample_frequency)\n",
        "    phasic_sc_array[i] = r\n",
        "    tonic_sc_array[i] = t\n",
        "  return phasic_sc_array, tonic_sc_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAF8KW3buLk0"
      },
      "source": [
        "Now, let's build a set of features for each type:\n",
        "\n",
        "1. sc_array_processed\n",
        "2. phasic_sc_array\n",
        "3. tonic_sc_array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqdFJD9cuKpo"
      },
      "source": [
        "!pip install tsfresh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvISZJnxun3W"
      },
      "source": [
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "import pandas as pd\n",
        "\n",
        "def prep_array_for_feature_extraction(input_array):\n",
        "  return_df = pd.DataFrame(input_array)\n",
        "  return_df = return_df.stack()\n",
        "  return_df.index.rename([ 'id', 'time' ], inplace = True )\n",
        "  return_df = return_df.reset_index()\n",
        "  return return_df\n",
        "\n",
        "def extract_all_features(sc_array_processed, phasic_sc_array, tonic_sc_array):\n",
        "  proccessed_features = extract_features(prep_array_for_feature_extraction(sc_array_processed), column_id=\"id\", column_sort=\"time\")\n",
        "  phasic_features = extract_features(prep_array_for_feature_extraction(phasic_sc_array), column_id=\"id\", column_sort=\"time\")\n",
        "  tonic_features = extract_features(prep_array_for_feature_extraction(tonic_sc_array), column_id=\"id\", column_sort=\"time\")\n",
        "  proccessed_features = impute(proccessed_features)\n",
        "  phasic_features = impute(phasic_features)\n",
        "  tonic_features = impute(tonic_features)\n",
        "  return proccessed_features, phasic_features, tonic_features\n",
        "\n",
        "def get_features(sc_array_processed, phasic_sc_array, tonic_sc_array):\n",
        "  proccessed_features, phasic_features, tonic_features = extract_all_features(sc_array_processed, phasic_sc_array, tonic_sc_array)\n",
        "  return proccessed_features, phasic_features, tonic_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlgi0M6P3xq6"
      },
      "source": [
        "Now, let's process the label values. \n",
        "\n",
        "Since we are classifying two classes, let's use a boolean representation for label counts. True represents high, False represents low. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTYsgozQx26m"
      },
      "source": [
        "def process_label(label_value):\n",
        "    if label_value > 5.5:\n",
        "        return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def get_labels():\n",
        "  valence_processed = np.empty(shape=(1280))\n",
        "  arousal_processed = np.empty(shape=(1280))\n",
        "  dominance_processed = np.empty(shape=(1280))\n",
        "  for i in range(len(valence_array_raw)):\n",
        "    valence_processed[i] = process_label(valence_array_raw[i])\n",
        "    arousal_processed[i] = process_label(arousal_array_raw[i])\n",
        "    dominance_processed[i] = process_label(dominance_array_raw[i])\n",
        "  print(\"Class imbalance (Full):\")\n",
        "  print(\"Valence levels: \", np.unique(valence_processed, return_counts=True))\n",
        "  print(\"Arousal levels: \", np.unique(arousal_processed, return_counts=True))\n",
        "  print(\"Dominance levels: \", np.unique(dominance_processed, return_counts=True))\n",
        "  return valence_processed, arousal_processed, dominance_processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYFYzmas5gRZ"
      },
      "source": [
        "Fit the extracted features to label values and store the resulting features in a format convenient for the classification tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0wDB1B5kP_"
      },
      "source": [
        "def get_selected_features(extracted_features, extracted_labels):\n",
        "  selected_feat_dict = {}\n",
        "  #First, let's combinte the features. \n",
        "  combined_features = pd.concat([extracted_features['processed_features'].add_prefix('o_'),\n",
        "                      extracted_features['phasic_features'].add_prefix('p_'),\n",
        "                      extracted_features['tonic_features'].add_prefix('t_')], axis=1)\n",
        "  \n",
        "  extracted_features['combined'] = combined_features\n",
        "\n",
        "  # Now, selected features with the tsfresh package\n",
        "  for feat_df_name, feat_df in extracted_features.items():\n",
        "    for label_array_name, label_array in extracted_labels.items():\n",
        "      selected_features = select_features(feat_df, pd.Series(label_array))\n",
        "      if len(selected_features.columns) > 0:\n",
        "        selected_feat_dict[feat_df_name + '_SELECTED-' + label_array_name] = {'features': selected_features,\n",
        "                                                                    'labels': label_array,\n",
        "                                                                    'features_count': len(selected_features.columns)}\n",
        "\n",
        "\n",
        "  return selected_feat_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwYn8KF8_pNM"
      },
      "source": [
        "Now, let's build the K-fold routine to test a bunch of classification algos on each of the feature-label combos in the selected_feat_dict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv5ndDsX5e4w"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\n",
        "def evaluate_experiments(selected_feat_dict):\n",
        "  classifiers = {'Dummy': DummyClassifier(),\n",
        "                'Random Forest': RandomForestClassifier(),\n",
        "                'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "                'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
        "                'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "                'GaussianNB': GaussianNB(),\n",
        "                'DecisionTree': DecisionTreeClassifier(),\n",
        "                'SVM': SVC(),\n",
        "                }\n",
        "  results_dict = {}\n",
        "  skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=9) \n",
        "  for feat_label_name, data_dict in selected_feat_dict.items():\n",
        "    X = data_dict['features']\n",
        "    X = X.to_numpy()\n",
        "    X = preprocessing.scale(X)\n",
        "    Y = data_dict['labels'] \n",
        "    for classifier_name, classifier_model in classifiers.items(): \n",
        "      accuracy_list = []\n",
        "      f1_list = []\n",
        "      random_guess_list = []\n",
        "      for train, test in skf.split(X, Y):\n",
        "        classifier = classifier_model\n",
        "        classifier.fit(X[train], Y[train])\n",
        "        pred = classifier.predict(X[test])\n",
        "        results = classification_report(Y[test], pred, output_dict=True)\n",
        "        accuracy_list.append(results['accuracy'])\n",
        "        f1_list.append(results['weighted avg']['f1-score'])\n",
        "      results_dict[feat_label_name + ' | ' + classifier_name] = {'Mean Accuracy': np.mean(accuracy_list),\n",
        "                                                                 'Accuracy STD': np.std(accuracy_list),\n",
        "                                                                 'Mean F1 Score': np.mean(f1_list),\n",
        "                                                                 'F1 Score STD': np.std(f1_list)}\n",
        "\n",
        "  return(results_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_Hhcf3KSYo"
      },
      "source": [
        "Experiment execution block (modify this)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNy7QNTj7Cba"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "sys.stdout = sys.__stdout__ # Unsilence the print statements. \n",
        "\n",
        "# First, let's define the hyper parameters:\n",
        "desired_sample_frequency = 10 #Hz, this probably will remain constant\n",
        "desired_signal_length_seconds = 10 # Last N second that we want to use for classification\n",
        "desired_signal_length = desired_signal_length_seconds * desired_sample_frequency # Samples in each signal\n",
        "cutoff_frequency = 1 # When filtering the raw SC signal, we choose the cutoff frequency for the lowpass filter. \n",
        "\n",
        "# Now, our folder directories \n",
        "DEAP_data_directory = \"/content/drive/My Drive/Engineering/Datasets/DEAP\"\n",
        "features_directory = DEAP_data_directory + \"/sc/features\"\n",
        "results_directory = DEAP_data_directory + \"/sc/results\"\n",
        "\n",
        "# Now, file names\n",
        "name_ending = \"_{length}s_{cutoff}hz.csv\".format(length=desired_signal_length_seconds, cutoff=str(cutoff_frequency).replace('.', ''))\n",
        "processed_features_name = \"/processed_features\" + name_ending\n",
        "phasic_features_name = \"/phasic_features\" + name_ending\n",
        "tonic_features_name = \"/tonic_features\" + name_ending\n",
        "results_file_name = \"/classic_ML_results\" + name_ending\n",
        "\n",
        "# Features files\n",
        "processed_features_file = features_directory + processed_features_name\n",
        "phasic_features_file = features_directory + phasic_features_name\n",
        "tonic_features_file = features_directory + tonic_features_name\n",
        "files = [processed_features_file, phasic_features_file, tonic_features_file]\n",
        "\n",
        "# Results file:\n",
        "results_file = results_directory + results_file_name\n",
        "\n",
        "# Let's check if the the feature file for the signal-length & frequency combo \n",
        "# exists, and use that instead of going through the processing routine\n",
        "files_exist = []\n",
        "for file in files:\n",
        "  if os.path.isfile(file):\n",
        "    files_exist.append(True)\n",
        "  else:\n",
        "    files_exist.append(False)\n",
        "\n",
        "if all(files_exist):\n",
        "  # Use these files\n",
        "  print(\"Freatures files for the signal-frequency combination exists. Using them...\")\n",
        "  proccessed_features = pd.read_csv(processed_features_file)\n",
        "  phasic_features = pd.read_csv(phasic_features_file)\n",
        "  tonic_features = pd.read_csv(tonic_features_file)\n",
        "else: \n",
        "  # Initiate feature extraction\n",
        "  # Step 0: Get raw data:\n",
        "  sc_array_raw, valence_array_raw, arousal_array_raw, dominance_array_raw = get_raw_data(DEAP_data_directory) \n",
        "  # Step 1: Get the wearable data, decompose it to desred signal length\n",
        "  # sc_array_processed\n",
        "  # phasic_sc_array\n",
        "  # tonic_sc_array\n",
        "  print(\"Processing raw data into a wearable format...\")\n",
        "  sc_array_processed = process_data(sc_array_raw, cutoff_frequency=cutoff_frequency,\n",
        "                                    desired_sample_frequency=desired_sample_frequency)\n",
        "  \n",
        "  print(\"Decomposing into phasic and tonic components using cvxEDA...\")\n",
        "  phasic_sc_array, tonic_sc_array = apply_cvx(sc_array_processed=sc_array_processed)\n",
        "\n",
        "  # Step 3: Cut the signals to desired length\n",
        "  print(\"Cutting the signals to the desired length\")\n",
        "  sc_data = np.empty(shape=(len(sc_array_processed), desired_signal_length))\n",
        "  phasic_sc_data = np.empty(shape=(len(phasic_sc_array), desired_signal_length))\n",
        "  tonic_sc_data = np.empty(shape=(len(tonic_sc_array), desired_signal_length))\n",
        "  for i in range(len(sc_array_processed)):\n",
        "    sc_data[i] = sc_array_processed[i][-desired_signal_length:]\n",
        "    phasic_sc_data[i] = phasic_sc_array[i][-desired_signal_length:]\n",
        "    tonic_sc_data[i] = tonic_sc_array[i][-desired_signal_length:]\n",
        "\n",
        "  # Step 4: Get the features\n",
        "  print(\"Extracting the features from processed, phasic and tonic signals.\")\n",
        "  processed_features, phasic_features, tonic_features = get_features(sc_array_processed=sc_data, \n",
        "                                                                    phasic_sc_array=phasic_sc_data, \n",
        "                                                                    tonic_sc_array=tonic_sc_data)\n",
        "  \n",
        "  # Step 5: Store the features into the files\n",
        "  print(\"Storing the files with the ending: \", name_ending)\n",
        "  processed_features.to_csv(processed_features_file)\n",
        "  phasic_features.to_csv(phasic_features_file)\n",
        "  tonic_features.to_csv(tonic_features_file)\n",
        "\n",
        "# Now, let's prepare the labels\n",
        "print(\"Preparing the labels\")\n",
        "valence_processed, arousal_processed, dominance_processed = get_labels()\n",
        "\n",
        "# Select Features\n",
        "print(\"Selecting the features using tsfresh\")\n",
        "extracted_features = {'processed_features': processed_features, \n",
        "                      'phasic_features': phasic_features, \n",
        "                      'tonic_features': tonic_features\n",
        "                      }\n",
        "\n",
        "extracted_labels = {'valence_processed': valence_processed,\n",
        "                    'arousal_processed': arousal_processed,\n",
        "                    'dominance_processed': dominance_processed\n",
        "                    }\n",
        "\n",
        "selected_feat_dict = get_selected_features(extracted_features=extracted_features,\n",
        "                                           extracted_labels=extracted_labels)\n",
        "\n",
        "# Test the classifiers and store the results\n",
        "print(\"Testing the classifiers\")\n",
        "results_df = evaluate_experiments(selected_feat_dict=selected_feat_dict)\n",
        "results_df = pd.DataFrame.from_dict(results_df)\n",
        "results_df = results_df.transpose()\n",
        "results_df.to_csv(results_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}